---
layout: post
title:  Implementing B+ Trees on Disk, Part 1 (Draft)
date:   2025-01-13 10:00
categories: database b+ tree
---

In this series we will be implementing a disk based B+ tree using OCaml. In this part I will provide a motivation for the challenge of implementing a disk based B+ tree, and then dig into the underlying storage manager that the higher level B+ tree algorithm will rely upon. In part 2 we will cover node layout on disk and serialization/deserialization functionality. Part 3 will cover insertion into the tree, and finally part 4 will cover deletion and some auxiliary functions derived from the implementation at that point. 

There are many resources on the basics of B+ trees and the associated algorithms for inserting and deleting from the tree, but they miss one crucial aspect: actually storing and maintaining the B+ tree on disk. It's easy enough to implement an in memory data structure and have it working while our program is running -- but how do we persist the information on disk when our program stops running or the computer shuts off? I will assume some familiarity with what a B+ tree is and why we need one in the first place. We won't get to the actual B+ tree insertion/deletion algorithms until later, when we have all the low level disk access and layout methods implemented. 

This is part of my OCaml_DB project, although the tutorial should be agnostic from any particular database, and hopefully you can follow along and hook it up to your own database. Also, even though I will be using OCaml, a lot of the code is imperative in nature and I think has a similar feeling to programming in a language like Rust or Go, with the added benefit of an exceptional type system and ease of reading compared to a language like Rust. Furthermore, I didn't consult any resources for how B+ trees are implemented in practice; this was my own attempt of designing an on disk B+ tree, so use it as inspiration to design your own if anything I do is particularly egregious, and also I would be very happy to hear about how you would design things differently. Future blog posts may include 1) comparing our implementation to existing B+ trees, and seeing where we can make some improvement with that knowledge and 2) showing how I hook up our B+ tree to OCaml_DB, and intertwining it with our transaction and buffer pool managers.

First off, here is a straw man design for making a disk based B+ tree to showcase the challenge we are really trying to solve. The "stupid" design would be as follows: when we are running a B+ tree operation, get the entire tree from disk at once and deserialize it it into an in memory representation of the tree. Then you would be able to write your code simply following any suitable pseudocode, without worrying about disk operations. At the end, you serialize and write it back to disk and you are done. There are a few clear problems with this approach. It requires reading the entire tree from disk, which is proporitonal to a linear scan on the original table in the first place. This defeats the entire purpose of having a B+ tree in the first place. Even if you can do it just one time and keep it cached in memory, the table you are indexing can be massive, either exceeding available RAM or taking up too much space. 

So we have to ensure we are keeping both look up time and associated memory footprint to a minimum; the two are related since the fewer disk accesses we have to make, the quicker the algorithm will run as well as having a lower associated memory footprint. This implies we want to keep the disk access localized to particular nodes during traversal in the algorithm. Suppose we have a degenerate case of a B+ tree that is just a binary search tree; it has one key and two pointers. The pseudocode for traversing the internal nodes of the B+ tree to get to the leaf level could look something like this: 

```
fun find_key key node_pointer = 
    node = read_from_disk node_pointer 
    match node.type with 
    | Internal -> 
        if key <= node.key:
        then 
            find_key key node.left_pointer 
        else
            find_key key node.right_pointer 
    | Leaf -> ...
```

And the traversal can be kicked off with a pointer to the root node: 

```
find_key key root_pointer
```

Instead of passing around an explicit node, we will be passing along "pointers" to where the nodes can be found on disk. For a traversal, we deserialize the node from disk and then use the in memory representation within our function logic. Instead of recursively calling the function on a node, we pass a child pointer to where the next node is located on disk. 

If we have to make some modification to a node, we just make sure we write back to disk to the location specified by the pointer. For example, if we have a function to insert a value in a leaf node (assuming there is space in the node):

```
fun insert_in_leaf key record_pointer leaf_pointer = 
    node = read_from_disk leaf_pointer
    // Insert (key, record_pointer) in appropriate location. 
    ...
    write_to_disk node leaf_pointer
```

This leads us to our first design decision: Our B+ tree will be stored in a file on disk. Each node of the tree will be a block of the file. The block size can be arbitrary; for simplicity we will assume it aligns with the unit of block access provided by the hardware. For something like a binary search tree, this would be extremely inefficient, since we would only have one key and two child pointers taking up an entire block. So we want to make sure we are padding as many key and pointer values into a block as possible; this will be discussed in part 2. This also works perfectly with the intended structure of a B+ tree; since we want to minimize the number of disk accesses, we want to increase the fanout of the tree as much as possible. 


I'm taking a bit of a meandering approach here in the discussion -- although we are not implementing the higher level yet, it's useful to get a sketch of the kind of things we will need available for us in order to implement the lower levels. This gives us an idea of what we need for our storage manager:
a file to store our b+ tree, and each block in the file corresponds to a node in our B+ tree. A simple interface for the B+ tree storage manager is based on block creation, update, and delete methods. 

Actually, we need to go one level lower even before we have our B+ tree storage manager. The storage manager will have a particular semantics that allows for allocating and deallocating blocks on disk, without worrying about underlying file operations as much. We need a layer of abstraction that will handle the low level file operations, so that we can focus on implementing the block manager API without having to worry about nitty gritty details like seeking to the correct position in a file. 

The file manager will be responsible for creating files and allowing us to perform low level read and write operations on the file with ease. It has two associated modules: a Block_id module and a Page module. Let's start with block_id's. This is a very simple datatype: it consists of just a file name and a number denoting a block offset. This is a "pointer" or a way for us to give a name to a location in a file. Since we are dealing with blocks in our B+ tree, we do not care about the granularity of a single byte, rather our locations are specific blocks within a file. 

The Page datatype will be our in memory buffer for storing blocks we read from disk. These are initialized to be the same as whatever our block size is. As a reminder for the rest of the post, just assume the block size is initialized once, and the same block size is used throughout. We can initialize our code with any block size we want, and it will work, but it can be kicked off by getting an OS specific size that is most efficient for reading and writing from disk. 

Here is our API for block_id's:
```
type t

val file_name : t -> string
val block_num : t -> int
val make : filename:string -> block_num:int -> t
val to_string : t -> string
val eq : t -> t -> bool
```

And an implementation: 

```
type t = string * int

let make ~filename ~block_num = (filename, block_num)
let file_name (filename, _) = filename
let block_num (_, block_num) = block_num
let to_string (s, n) = Printf.sprintf "%s, %d" s n
let eq (s1, n1) (s2, n2) = s1 = s2 && n1 = n2
```

And here is our API for the Page datatype:

```
type t

val make : block_size:int -> t
val from_bytes : bytes -> t
val get_int32 : t -> int -> int32
val set_int32 : t -> int -> int32 -> unit
val contents : t -> bytes
val get_bytes : t -> int -> bytes
val set_bytes : t -> int -> bytes -> unit
val get_string : t -> int -> string
val set_string_raw : t -> int -> string -> unit
val get_string_raw : t -> int -> int -> string
val set_string : t -> int -> string -> unit
val max_len : int -> int
val ( = ) : t -> t -> bool
val zero_out : t -> unit
```

With an implementation:

```
type t = bytes

let make ~block_size = Bytes.make block_size '\000'
let from_bytes b = b
let get_int32 page offset = Bytes.get_int32_ne page offset
let set_int32 page offset n = Bytes.set_int32_ne page offset n
let contents page = page

let get_bytes page offset =
  let len = Int32.to_int (get_int32 page offset) in
  Bytes.sub page (offset + 4) len

let set_bytes page offset b =
  assert (offset + 4 + Bytes.length b <= Bytes.length page);
  let len = Bytes.length b in
  let _ = set_int32 page offset (Int32.of_int len) in
  Bytes.blit b 0 page (offset + 4) len

let set_string_raw page offset s = 
  let b = Bytes.of_string s in 
  let len = Bytes.length b in 
  Bytes.blit b 0 page offset len

let get_string_raw page offset n = Bytes.to_string (Bytes.sub page offset n)

let get_string page offset = Bytes.to_string (get_bytes page offset)
let set_string page offset s = set_bytes page offset (Bytes.of_string s)

(* only ascii encoding for now *)
let max_len l = 4 + l
let ( = ) = Bytes.equal
let zero_out page = Bytes.fill page 0 (Bytes.length page) '\000'
```






