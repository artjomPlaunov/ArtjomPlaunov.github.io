---
layout: post
title:  Implementing B+ Trees on Disk, Part 1 (Draft)
date:   2025-01-13 10:00
categories: database b+ tree
---

In this series we will be implementing a disk based B+ tree using OCaml. In this part I will provide a motivation for the challenge of implementing a disk based B+ tree, and then dig into the underlying storage manager that the higher level B+ tree algorithm will rely upon. In part 2 we will cover node layout on disk and serialization/deserialization functionality. Part 3 will cover insertion into the tree, and finally part 4 will cover deletion and some auxiliary functions derived from the implementation at that point. 

There are many resources on the basics of B+ trees and the associated algorithms for inserting and deleting from the tree, but they miss one crucial aspect: actually storing and maintaining the B+ tree on disk. It's easy enough to implement an in memory data structure and have it working while our program is running -- but how do we persist the information on disk when our program stops running or the computer shuts off? I will assume some familiarity with what a B+ tree is and why we need one in the first place. We won't get to the actual B+ tree insertion/deletion algorithms until later, when we have all the low level disk access and layout methods implemented. 

This is part of my OCaml_DB project, although the tutorial should be agnostic from any particular database, and hopefully you can follow along and hook it up to your own database. Also, even though I will be using OCaml, a lot of the code is imperative in nature and I think has a similar feeling to programming in a language like Rust or Go, with the added benefit of an exceptional type system and ease of reading compared to a language like Rust. Furthermore, I didn't consult any resources for how B+ trees are implemented in practice; this was my own attempt of designing an on disk B+ tree, so use it as inspiration to design your own if anything I do is particularly egregious, and also I would be very happy to hear about how you would design things differently. Future blog posts may include 1) comparing our implementation to existing B+ trees, and seeing where we can make some improvement with that knowledge and 2) showing how I hook up our B+ tree to OCaml_DB, and intertwining it with our transaction and buffer pool managers.

First off, here is a straw man design for making a disk based B+ tree to showcase the challenge we are really trying to solve. The "stupid" design would be as follows: when we are running a B+ tree operation, get the entire tree from disk at once and deserialize it it into an in memory representation of the tree. Then you would be able to write your code simply following any suitable pseudocode, without worrying about disk operations. At the end, you serialize and write it back to disk and you are done. There are a few clear problems with this approach. It requires reading the entire tree from disk, which is proporitonal to a linear scan on the original table in the first place. This defeats the entire purpose of having a B+ tree in the first place. Even if you can do it just one time and keep it cached in memory, the table you are indexing can be massive, either exceeding available RAM or taking up too much space. 

So we have to ensure we are keeping both look up time and associated memory footprint to a minimum; the two are related since the fewer disk accesses we have to make, the quicker the algorithm will run as well as having a lower associated memory footprint. This implies we want to keep the disk access localized to particular nodes during traversal in the algorithm. Suppose we have a degenerate case of a B+ tree that is just a binary search tree; it has one key and two pointers. The pseudocode for traversing the internal nodes of the B+ tree to get to the leaf level could look something like this: 

```
fun find_key key node_pointer = 
    node = read_from_disk node_pointer 
    match node.type with 
    | Internal -> 
        if key <= node.key:
        then 
            find_key key node.left_pointer 
        else
            find_key key node.right_pointer 
    | Leaf -> ...
```

And the traversal can be kicked off with a pointer to the root node: 

```
find_key key root_pointer
```

Instead of passing around an explicit node, we will be passing along "pointers" to where the nodes can be found on disk. For a traversal, we deserialize the node from disk and then use the in memory representation within our function logic. Instead of recursively calling the function on a node, we pass a child pointer to where the next node is located on disk. 

If we have to make some modification to a node, we just make sure we write back to disk to the location specified by the pointer. For example, if we have a function to insert a value in a leaf node (assuming there is space in the node):

```
fun insert_in_leaf key record_pointer leaf_pointer = 
    node = read_from_disk leaf_pointer
    // Insert (key, record_pointer) in appropriate location. 
    ...
    write_to_disk node leaf_pointer
```

This leads us to our first design decision: Our B+ tree will be stored in a file on disk. Each node of the tree will be a block of the file. The block size can be arbitrary; for simplicity we will assume it aligns with the unit of block access provided by the hardware. For something like a binary search tree, this would be extremely inefficient, since we would only have one key and two child pointers taking up an entire block. So we want to make sure we are padding as many key and pointer values into a block as possible; this will be discussed in part 2. This also works perfectly with the intended structure of a B+ tree; since we want to minimize the number of disk accesses, we want to increase the fanout of the tree as much as possible. 


I'm taking a bit of a meandering approach here in the discussion -- although we are not implementing the higher level yet, it's useful to get a sketch of the kind of things we will need available for us in order to implement the lower levels. This gives us an idea of what we need for our storage manager:
a file to store our b+ tree, and each block in the file corresponds to a node in our B+ tree. A simple interface for the B+ tree storage manager is based on block creation, update, and delete methods. 

Actually, we need to go one level lower even before we have our B+ tree storage manager. The storage manager will have a particular semantics that allows for allocating and deallocating blocks on disk, without worrying about underlying file operations as much. We need a layer of abstraction that will handle the low level file operations, so that we can focus on implementing the block manager API without having to worry about nitty gritty details like seeking to the correct position in a file. 

The file manager will be responsible for creating files and allowing us to perform low level read and write operations on the file with ease. It has two associated modules: a Block_id module and a Page module. Let's start with block_id's. This is a very simple datatype: it consists of just a file name and a number denoting a block offset. This is a "pointer" or a way for us to give a name to a location in a file. Since we are dealing with blocks in our B+ tree, we do not care about the granularity of a single byte, rather our locations are specific blocks within a file. 

The Page datatype will be our in memory buffer for storing blocks we read from disk. These are initialized to be the same as whatever our block size is. As a reminder for the rest of the post, just assume the block size is initialized once, and the same block size is used throughout. We can initialize our code with any block size we want, and it will work, but it can be kicked off by getting an OS specific size that is most efficient for reading and writing from disk. 

Here is our API for block_id's:
```
type t

val file_name : t -> string
val block_num : t -> int
val make : filename:string -> block_num:int -> t
val to_string : t -> string
val eq : t -> t -> bool
```

And an implementation: 

```
type t = string * int

let make ~filename ~block_num = (filename, block_num)
let file_name (filename, _) = filename
let block_num (_, block_num) = block_num
let to_string (s, n) = Printf.sprintf "%s, %d" s n
let eq (s1, n1) (s2, n2) = s1 = s2 && n1 = n2
```

And here is our API for the Page datatype:

```
type t

val make : block_size:int -> t
val from_bytes : bytes -> t
val get_int32 : t -> int -> int32
val set_int32 : t -> int -> int32 -> unit
val contents : t -> bytes
val get_bytes : t -> int -> bytes
val set_bytes : t -> int -> bytes -> unit
val get_string : t -> int -> string
val set_string_raw : t -> int -> string -> unit
val get_string_raw : t -> int -> int -> string
val set_string : t -> int -> string -> unit
val max_len : int -> int
val ( = ) : t -> t -> bool
val zero_out : t -> unit
```

With an implementation:

```
type t = bytes

let make ~block_size = Bytes.make block_size '\000'
let from_bytes b = b
let get_int32 page offset = Bytes.get_int32_ne page offset
let set_int32 page offset n = Bytes.set_int32_ne page offset n
let contents page = page

let get_bytes page offset =
  let len = Int32.to_int (get_int32 page offset) in
  Bytes.sub page (offset + 4) len

let set_bytes page offset b =
  assert (offset + 4 + Bytes.length b <= Bytes.length page);
  let len = Bytes.length b in
  let _ = set_int32 page offset (Int32.of_int len) in
  Bytes.blit b 0 page (offset + 4) len

let set_string_raw page offset s = 
  let b = Bytes.of_string s in 
  let len = Bytes.length b in 
  Bytes.blit b 0 page offset len

let get_string_raw page offset n = Bytes.to_string (Bytes.sub page offset n)

let get_string page offset = Bytes.to_string (get_bytes page offset)
let set_string page offset s = set_bytes page offset (Bytes.of_string s)

(* only ascii encoding for now *)
let max_len l = 4 + l
let ( = ) = Bytes.equal
let zero_out page = Bytes.fill page 0 (Bytes.length page) '\000'
```

The Page is really a wrapper for the OCaml bytes datatype, so that we can provide a constrained API for our page functionality on top of a mutable Bytes array. For example, the set_string_raw method takes a page type that has been initialize already, an offset within the page, and a string. It converts the string into bytes, and uses an underlying Bytes.blit method that sets the bytes at an offset within the page. The methods for this are all fairly straighforward. One peculiarity here is the get_int32 and set_int32 functions. OCaml ints aren't actually 32 bits, they are 31 bits with the extra bit used for the OCaml runtime environment. This poses a bit of an issue if we want to encode everything as 32 bits, on disk. So the choice made here was to use the OCaml int32 datatype at the page level, even though a the rest of the database uses regular ocaml ints (since there aren't cases where we actually need values that use up all 32 bits). 

Also, we use the get_int32_ne and set_int32_ne functions, where ne stands for native encoding of big or little endianess. We have our int32 on the programmatic level, and this will ensure that whenever we get an int or set an int it will do it in native encoding. This makes the file on disk non portable between different machines, but for any particular DB instance running on any particular machine, it will produce the correct results, since we always serialize to native encoding and deserialize back from native encoding. To make the disk file portable between machines, we would have to pick a specific endianess and make sure we always serialize/deserialize ints with that endianess at this point of the code. For now the database is not distributed so it isn't an issue yet, but it isn't a complicated fix to make later on, especially since we can maintain the Page API to users. 

Now we can implement our file manager. Some stuff in here is specific to having it integrated with the database, so I will gloss over those components and just go over the core functionality we need for building our B+ tree storage manager. Here is the API for the file manager:

```
type t

val make : db_dirname:string -> block_size:int -> t
val is_new : t -> bool
val get_blocksize : t -> int
val get_file : t -> string -> Unix.file_descr
val read : t -> Block_id.t -> Page.t -> unit
val write : t -> Block_id.t -> Page.t -> unit
val append : t -> string -> Block_id.t
val size : t -> string -> int
```

Let's dissect the API a bit before I provide the implementation. The constructor function is a bit specific to our DB, since it takes a directory where we will want to put our files. The more important aspect is the block_size parameter, which will then be used throughout our code as the block size for everything. No files are actually created with the constructor. Internally, the make function will also initialize the file manager with a hashtable of active file descriptors for each file in the database. 

get_file can be used to create a new file; if it doesn't exist then a new file is created and a file descriptor is returned. If it already exists, then we just return the file descriptor. The read, write, and append functions will be more relevant for us; in fact this is already very similar to what we want in our B+ tree storage manager API. The reason we will add an extra layer is that we want a similar API but with some different semantics for what is going on within the file. The read function takes a logical block address (The Block_id type) and a page, and reads data from that block into the page. Write will take a page and write it to disk at an address. Append takes an existing file in the file manager and appends a block at the end. 

This is strikingly similar to what we will want for the B+ tree, however there are some block management semantics specific to the B+ tree that justifies adding an extra abstraction layer on top of the file manager (We will see this shortly). We can think of the file manager as just reading, writing, appending blocks without any concern for the file layout or what it means. 


Although the file manager was implemented using Edward Sciore's design from "Database Design and Implementation", for completeness I will include the implementation here, since this is truly the lowest level at which we are interacting with the disk, and the whole purpose of this blog post is to show how to implement the B+ tree on disk. Here is the implementation: 

```
type t = {
  is_new : bool;
  db_dir : Unix.dir_handle;
  db_dirname : string;
  block_size : int;
  open_files : (string, Unix.file_descr) Hashtbl.t;
}

exception InitDbErr
exception FileMgrReadErr

let rec clean_temp_dirs db_dirname db_dir =
  try
    let cur_file = Unix.readdir db_dir in
    if String.length cur_file >= 4 && String.sub cur_file 0 4 = "temp" then (
      Sys.remove (Filename.concat db_dirname cur_file);
      clean_temp_dirs db_dirname db_dir)
    else clean_temp_dirs db_dirname db_dir
  with End_of_file -> ()

(* File Manager constructor. *)
let make ~db_dirname ~block_size =
  (* Create open file handler for DB directory *)
  let db_dir, is_new =
    try
      let stat = Unix.stat db_dirname in
      if stat.st_kind = Unix.S_DIR then (Unix.opendir db_dirname, false)
      else raise InitDbErr
    with
    (* If it doesn't exist already, create it. *)
    | Unix.Unix_error (Unix.ENOENT, _, _) ->
        let _ = Unix.mkdir db_dirname 0o755 in
        (Unix.opendir db_dirname, true)
    | _ -> raise InitDbErr
  in
  (* Remove leftover temporary tables. *)
  clean_temp_dirs db_dirname db_dir;
  Unix.rewinddir db_dir;
  let open_files = Hashtbl.create 10 in
  { is_new; db_dir; db_dirname; block_size; open_files }

let is_new file_mgr = file_mgr.is_new
let get_blocksize file_mgr = file_mgr.block_size

let get_file file_mgr filename =
  match Hashtbl.find_opt file_mgr.open_files filename with
  | Some fd -> fd
  | None ->
      let full_path = Filename.concat file_mgr.db_dirname filename in
      let fd = Unix.openfile full_path Unix.[ O_RDWR; O_CREAT; O_SYNC ] 0o755 in
      Hashtbl.add file_mgr.open_files filename fd;
      fd

let read file_mgr block page =
  let fd = get_file file_mgr (Block_id.file_name block) in
  let offset = Block_id.block_num block * file_mgr.block_size in
  let _ = Unix.lseek fd offset SEEK_SET in
  let n = Unix.read fd (Page.contents page) 0 file_mgr.block_size in
  if n = 0 then Page.zero_out page else ()

(* Since Unix.write doesn't guarantee writing all n bytes,
   we have a helper function to repeatedly call write until we
   have written all n bytes.

   Note there is a possible uncaught exception here, if anything
   goes wrong with writing.
*)
let rec write_n fd page offset n =
  if n = 0 then ()
  else
    let bytes_written = Unix.write fd page offset n in
    write_n fd page (offset + bytes_written) (n - bytes_written)

let write file_mgr block page =
  let fd = get_file file_mgr (Block_id.file_name block) in
  let offset = Block_id.block_num block * file_mgr.block_size in
  let _ = Unix.lseek fd offset SEEK_SET in
  write_n fd (Page.contents page) 0 file_mgr.block_size

let size file_mgr filename =
  let _ = get_file file_mgr filename in
  let full_path = Filename.concat file_mgr.db_dirname filename in
  let stat = Unix.stat full_path in
  stat.st_size / file_mgr.block_size

let append file_mgr filename =
  let block_num = size file_mgr filename in
  let block = Block_id.make ~filename ~block_num in
  let b = Bytes.make file_mgr.block_size '\000' in
  let fd = get_file file_mgr filename in
  let _ = Unix.lseek fd (block_num * file_mgr.block_size) SEEK_SET in
  write_n fd b 0 file_mgr.block_size;
  block
```

Well, this definitely wont run on windows. Note the use of the OCaml Unix library, which provides us with OS system calls to handle the file operations. I will briefly cover the write function. We get the file descriptor from our file manager, then using the block offset in the file (the block we are trying to write to), we calculate the byte offset within the file. Then we use the lseek system call to position at the start of where we want to write to. Then we call an auxiliary write_n function, which recursively writes until all n bytes are written. This is due to the semantics of the Unix.write system call: it is not guaranteed that all n bytes will be written at once, rather we get the number of bytes successfully written, then keep trying to write the other bytes. Note there is an uncaught exception here if anything goes wrong. 

Now we can get to writing our B+ tree storage manager. The main difference with the file manager is 1) the storage manager will only be responsible for one file in which the specific B+ tree is being stored, and it will handle deletion with a free list to use previously deleted blocks whenever we need to create. Whereas the file manager has a one to many relation between the file manager and the files, each storage manager instance has a one to one relationship with its own file. 

Here is our API for the storage manager: 

```
type t 

val make    :   File_manager.t -> string -> t
val append  :   t -> Page.t -> Block_id.t  
val delete  :   t -> Block_id.t -> unit
val update  :   t -> Block_id.t -> Page.t -> unit
val update_block_num :  t -> int -> Page.t -> unit
val get_block : t -> int -> Page.t
```

This is very similar to the file manager, with the difference being the delete function and an internal free list from which we can fetch new nodes whenever we append. When we delete a node from the tree (hence deleting the associated block using this API), the block is added to a free list for future use.

If we didn't have a free list, then everytime we deleted a node we would have three basic options: 1) The file is append only, and deleted blocks will sit around taking up space or 2) we explicitly shift everything after a deleted block, which is costly, or 3) we can take a used block at the end, after the deleted block, and put it in place of the deleted block; however this would require the additional complexity of pointer manipulation, to make sure that any nodes pointing to that block are updated to the new location; this means explicitly maintaining this information and performing additional disk reads and writes to update the pointers. 

The downside with the free list approach is in the kind of scenario where we have many inserts followed by many deletes, which will produce a lot of unused blocks in the file, hanging around in the free list. To remedy this we may add a garbage collection step in the future. 

Our storage manager will always reserve the first block in the file to point to the first element of the free list. Initially, the head block is initialized to 0, since it is not pointing anywhere. Here is what the initialization function looks like: 

```
let make ~file_manager ~storage_file = 
  let block_size = File_manager.get_blocksize file_manager in 
  let head_page = Page.make ~block_size in 
  let block = Block_id.make ~filename:storage_file ~block_num:0 in 
  if File_manager.size file_manager storage_file = 0 then (
    Page.set_int32 head_page 0 (Int32.of_int 0);
    File_manager.write file_manager block head_page
  ) else
    File_manager.read file_manager block head_page;
  {file_manager; storage_file; head_page}
```

Note that this may be initialized to an existing B+ tree in the file, i.e. on startup. If the associated file has 0 blocks in it, then we initialize the first block to just contain all 0's, then write it to disk. Otherwise the file is empty, and we read the first block into a page, which we keep around fresh in memory. Since we're keeping an active copy in memory, we have to ensure that we keep it updated while we perform operations. 

Next, let's look into how we can append new blocks to our storage manager. Here is an example scenario from the test cases: 

Assume the storage manager starts out empty, so we just have block 0 which doesn't point anywhere. The contents of our storage manager are as follows:

```
Block 0: 0
```

Then, let's append 5 blocks. We'll put the random value 42 in each block. Note that the block numbering scheme for block N is really the N'th offset block, or if we are numbering the blocks from 1 it is the (N+1)st block in the file. So block 5 is at offset 5, but if we number from 1 it is the 6th block in the file. We will stick to offsets. 

```
Block 0: 0
Block 1: 42
Block 2: 42
Block 3: 42
Block 4: 42
Block 5: 42
```

Then we delete block 5:

```
Block 0: 5
Block 1: 42
Block 2: 42
Block 3: 42
Block 4: 42
Block 5: 0

Free list: 5 
```

Now block 0 is pointing to block 5, and block 5 is the end of the free list and has the value 0 to denote it is the end of the free list (not to denote that block 0 is itself the next element in the free list, as block 0 will never be used for storage). 

Let's now delete block 2:

```
Block 0: 2
Block 1: 42
Block 2: 5
Block 3: 42
Block 4: 42
Block 5: 0

Free list: 2 -> 5
```

What happened here? When we add a new element to the free list, we want to always append it to the front of the list to make it an O(1) operation in terms of disk reads and writes. What happened here was we inspected the current head of the list, which points to 5. We overwrite 5 into block 2, which is the same as inserting it into the front of the list; then we have block 0 point to block 2 instead of block 5. Since we are always keeping block 0 in memory, this means we only have to perform one disk read and write to update block 2. 

Let's delete blocks 1 and 3 in that order:

```
Block 0: 3
Block 1: 2
Block 2: 5
Block 3: 1
Block 4: 42
Block 5: 0

Free list: 3 -> 1 -> 2 -> 5
```

Now if we want to append a new block, we can fetch from the free list. We will append the value 9999 in a new block:

```
Block 0: 1
Block 1: 2
Block 2: 5
Block 3: 9999
Block 4: 42
Block 5: 0

Free list: 1 -> 2 -> 5
```

Before the insert operation, 3 was the head of the free list. So we fetched block 3, which was pointing to block 1 in the free list. We update the root node to point to block 1, and insert our new value 9999 into block 3. 






